{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PROCESADO YOUTUBE**"
      ],
      "metadata": {
        "id": "SJOAQsYy2-81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Preparación del entorno**"
      ],
      "metadata": {
        "id": "foWSSkKT3Ei4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  import pandas as pd\n",
        "except:\n",
        "  !pip install google-cloud-storage\n",
        "  !pip install pandas\n",
        "\n",
        "!pip install emoji google-api-python-client unidecode num2words\n",
        "\n",
        "from google.cloud import storage\n",
        "import emoji\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import string\n",
        "import regex as re\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from unidecode import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.utils import resample\n",
        "from num2words import num2words\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o_Or8AJIoOu",
        "outputId": "e8809107-8ff3-42ac-9470-59c35f2e1dbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.84.0)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Collecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2024.2.2)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=07b70ceee477f503009a6fcded5d3678eb309236bb62f43cf9b386d9b51558b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, unidecode, num2words, emoji\n",
            "Successfully installed docopt-0.6.2 emoji-2.12.1 num2words-0.5.13 unidecode-1.3.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def descargar_de_bucket_origen(nombre_bucket_origen, nombre_blob_origen, nombre_archivo_destino):\n",
        "    cliente = storage.Client()\n",
        "    bucket = cliente.bucket(nombre_bucket_origen)\n",
        "    blob = bucket.blob(nombre_blob_origen)\n",
        "    blob.download_to_filename(nombre_archivo_destino)\n",
        "    print(f\"Archivo {nombre_blob_origen} descargado como {nombre_archivo_destino}.\")\n",
        "\n",
        "nombre_bucket_origen = 'ds-edw-raw-d1655b14'\n",
        "nombre_bucket_procesado = 'ds-edw-processed-d1655b14'\n",
        "nombre_blob_origen = 'CSV/comentarios.csv'\n",
        "ruta_descarga_local = 'comentarios.csv'\n",
        "descargar_de_bucket_origen(nombre_bucket_origen, nombre_blob_origen, ruta_descarga_local)\n",
        "\n",
        "input_file = pd.read_csv(ruta_descarga_local)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFKbi18ocU7b",
        "outputId": "dda57b7c-7f2c-487e-e2af-3a9d9d7f98c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo CSV/comentarios.csv descargado como comentarios.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Pipeline**"
      ],
      "metadata": {
        "id": "rdV1mssn38TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(input_file, output_file):\n",
        "    # Cargar datos desde el archivo CSV\n",
        "    comments = pd.read_csv(input_file)\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F3B5-\\U0001F3EF\"\n",
        "                               u\"\\U0001F700-\\U0001F77F\"\n",
        "                               u\"\\U0001F7E0-\\U0001F7EB\"\n",
        "                               u\"\\U0001F0A0-\\U0001F0FF\"\n",
        "                               u\"\\U0001F000-\\U0001F02F\"\n",
        "                               u\"\\U0001F030-\\U0001F09F\"\n",
        "                               u\"\\U0001F200-\\U0001F2FF\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    def emoji_processed(df):\n",
        "        def extract_and_count_emojis(text):\n",
        "            if isinstance(text, str):\n",
        "                return emoji_pattern.findall(text)\n",
        "            return []\n",
        "\n",
        "        emoji_list = []\n",
        "        for index, row in df.iterrows():\n",
        "            emojis = extract_and_count_emojis(row['Comentario'])\n",
        "            if emojis:\n",
        "                for emoji in emojis:\n",
        "                    emoji_list.append({'emoji': emoji})\n",
        "\n",
        "        all_emoji = pd.DataFrame(emoji_list, columns=['emoji'])\n",
        "\n",
        "        # Calcular el porcentaje de aparición de cada emoji\n",
        "        emoji_counts = all_emoji['emoji'].value_counts(normalize=True) * 100\n",
        "        emoji_counts = emoji_counts.rename_axis('emoji').reset_index(name='% del total')\n",
        "        emoji_counts['% del total'] = emoji_counts['% del total'].round(2)\n",
        "\n",
        "        # Ordenar los emojis por porcentaje de mayor a menor\n",
        "        emoji_counts.sort_values(by='% del total', ascending=False, inplace=True)\n",
        "\n",
        "        # Guardar el DataFrame ordenado en un archivo CSV\n",
        "        emoji_counts.to_csv(\"new_emoji.csv\", index=False)\n",
        "\n",
        "    def remove_emojis(text):\n",
        "        if isinstance(text, str):\n",
        "            return emoji_pattern.sub('', text)\n",
        "        return text\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = remove_emojis(text)\n",
        "        if isinstance(text, str):\n",
        "            text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)\n",
        "            text = unidecode(text)\n",
        "            text = text.lower()\n",
        "            text = text.replace('!', '')\n",
        "            words = [num2words(word, lang='es') if word.isdigit() else word for word in text.split()]\n",
        "            text = ' '.join(words)\n",
        "        return text\n",
        "\n",
        "    # Procesar emojis y guardar en archivo CSV\n",
        "    emoji_processed(comments)\n",
        "\n",
        "    # Aplicar la función de limpieza de texto a la columna 'Comentario'\n",
        "    comments['new_processed'] = comments['Comentario'].apply(clean_text)\n",
        "    comments.drop('Comentario', axis=1, inplace=True)\n",
        "    comments.replace('', np.nan, inplace=True)\n",
        "    comments = comments.dropna()\n",
        "\n",
        "    # Guardar el DataFrame procesado en un archivo CSV\n",
        "    comments.to_csv(output_file, index=False)\n",
        "    return comments\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = 'comentarios.csv'\n",
        "    output_file = 'new_processed.csv'\n",
        "\n",
        "    processed_data = preprocess_data(input_file, output_file)\n",
        "\n",
        "    print(f'Los datos preprocesados han sido guardados en: {output_file}')\n",
        "\n",
        "    processed_data = pd.read_csv(output_file)\n",
        "    processed_data.info()\n",
        "    print(\".\" * 20)\n",
        "\n",
        "    for index, row in processed_data.head(5).iterrows():\n",
        "        print(\"Autor:\", row['Autor'])\n",
        "        print(\"Comentario:\", row['new_processed'])\n",
        "        print()\n",
        "\n",
        "    new_emoji = pd.read_csv('new_emoji.csv')\n",
        "    print(\".\" * 20)\n",
        "    print(new_emoji.head(6))\n",
        "\n",
        "\n",
        "def subir_a_bucket_destino(nombre_bucket_procesado, nombre_archivo_destino, nombre_blob_destino):\n",
        "    cliente = storage.Client()\n",
        "    bucket = cliente.bucket(nombre_bucket_procesado)\n",
        "    blob = bucket.blob(nombre_blob_destino)\n",
        "    # Verificar si el archivo ya existe en el bucket\n",
        "    if blob.exists():\n",
        "        print(f\"El archivo {nombre_blob_destino} ya existe en el bucket {nombre_bucket_procesado} y no se ha subido por no sobreescribirlo y preservar los datos, pon otro nombre o solicita ayuda al administrador del bucket.\")\n",
        "    else:\n",
        "        blob.upload_from_filename(nombre_archivo_destino)\n",
        "        print(f\"Archivo {nombre_archivo_destino} subido a {nombre_blob_destino} en el bucket {nombre_bucket_procesado}.\")\n",
        "\n",
        "\n",
        "nombre_bucket_procesado = 'ds-edw-processed-d1655b14'\n",
        "nombre_blob_destino = 'CSV/new_processed.csv'\n",
        "subir_a_bucket_destino(nombre_bucket_procesado, output_file, nombre_blob_destino)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jwGRXer37Xa",
        "outputId": "2ea24d29-4996-4041-9d06-b354ffeaa51a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los datos preprocesados han sido guardados en: new_processed.csv\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2001 entries, 0 to 2000\n",
            "Data columns (total 2 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Autor          2001 non-null   object\n",
            " 1   new_processed  2001 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 31.4+ KB\n",
            "....................\n",
            "Autor: @mipalacioazul4191\n",
            "Comentario: el mismo tema que trata la pelicula la llave maestra me suena muy probable\n",
            "\n",
            "Autor: @marinahernandezhernandez9814\n",
            "Comentario: teneis razon en decir que todo es mentira incluso vuestros programas\n",
            "\n",
            "Autor: @marinahernandezhernandez9814\n",
            "Comentario: pero tu no tenias por tu vida por karles torah\n",
            "\n",
            "Autor: @bellaguerra3183\n",
            "Comentario: increible pero me lo imaginaba\n",
            "\n",
            "Autor: @samadhi7453\n",
            "Comentario: esse povo nao morre troca de pele\n",
            "\n",
            "....................\n",
            "  emoji  % del total\n",
            "0     😮         7.96\n",
            "1     😊         4.66\n",
            "2     😂         4.66\n",
            "3     🌎         2.72\n",
            "4   😂😂😂         2.52\n",
            "5     😅         2.33\n",
            "El archivo CSV/new_processed.csv ya existe en el bucket ds-edw-processed-d1655b14 y no se ha subido por no sobreescribirlo y preservar los datos, pon otro nombre o solicita ayuda al administrador del bucket.\n",
            "El archivo CSV/new_emoji.csv ya existe en el bucket ds-edw-processed-d1655b14 y no se ha subido por no sobreescribirlo y preservar los datos, pon otro nombre o solicita ayuda al administrador del bucket.\n"
          ]
        }
      ]
    }
  ]
}